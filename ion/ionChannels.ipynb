{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport os\nfrom sklearn.metrics import f1_score\nimport graphviz\nfrom sklearn import tree\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import VarianceThreshold","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"test = pd.read_csv('../input/data-without-drift/test_clean.csv')[['time', 'signal']]\ntrain = pd.read_csv('../input/data-without-drift/train_clean.csv')[['time', 'signal', 'open_channels']]\ntrain.shape,test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm.notebook import tqdm\ntrain1 = np.asarray(train[['signal','open_channels']])\ntrain_dict = {}\nfor sig,chan in tqdm(train1):\n    temp = []\n    try:\n        temp = train_dict[sig]\n        temp.append(chan)\n    except KeyError:\n        temp.append(chan)\n    finally:\n        train_dict[sig] = temp","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**len(train_dict)** = 1967506 i.e. ~2Million unique signal values  "},{"metadata":{"trusted":true},"cell_type":"code","source":"print('len(train_dict)',len(train_dict)) \nsns.distplot(list(train_dict.keys()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_info_dict={}\nfrom collections import Counter\nfor key,value in train_dict.items():\n    train_info_dict[key] = Counter(value)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"counter = 0\nspecial_signals={}\nfor key,value in train_info_dict.items():\n    if len(value)>1:\n        special_signals[key] = value\n        counter+=1\n        #print(key,value)\nprint(\"No of signal values from Batch-3 until Batch-10 that overlap:\",counter)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"channel_probs = []\nfor sig,chan_dict in special_signals.items():\n    for i,j in chan_dict.items():\n        #print(sig,i,np.round(j/sum(chan_dict.values()),4))\n        channel_probs.append((sig,i,np.round(j/sum(chan_dict.values()),4)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_channel_probs = pd.DataFrame(channel_probs)\ndf_channel_probs.columns=['signal','open_channels','prob']\ndf_channel_probs.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_channel_probs.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = pd.merge(train, df_channel_probs, how='left', on=['signal','open_channels'])\nresult = result.fillna(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### If a particular signal value doesn't facilitate opening the same number of channels every time, then \"just\" the signal is not sufficient to determine the number of channels opening.  \n## Can I treat the 42704 signals that overlap as outliers??? Or perhaps assign them probability"},{"metadata":{"trusted":true},"cell_type":"code","source":"# mean = []\n# var = []\n# for i in range(11):\n#     sns.distplot(train_dict[i],axlabel='electrical signal values')\n#     mean.append(np.mean(train_dict[i]))\n#     var.append(np.std(train_dict[i])*np.std(train_dict[i]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for no_of_channels in range(11):\n#     print('no_of_channels:',no_of_channels,'| percent of overlapped signal',100*np.round(len(np.unique(train_dict[no_of_channels]))/len(train_dict[no_of_channels]),4),'%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"no_of_channels: 0 | percent of overlapped signal 18.4%    \nno_of_channels: 1 | percent of overlapped signal 46.43%  \nno_of_channels: 2 | percent of overlapped signal 44.11%  \nno_of_channels: 3 | percent of overlapped signal 45.91%  \nno_of_channels: 4 | percent of overlapped signal 52.98%  \nno_of_channels: 5 | percent of overlapped signal 57%  \nno_of_channels: 6 | percent of overlapped signal 56.46%  \nno_of_channels: 7 | percent of overlapped signal 53.6%    \nno_of_channels: 8 | percent of overlapped signal 53.78%  \nno_of_channels: 9 | percent of overlapped signal 56.61%  \nno_of_channels:10 | percent of overlapped signal 72.58%  \n\n**inference:** for 0 channels we can only be 72% sure during prediction, for 1 channels we can only be 54% sure during prediction, so on and so forth"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(test.signal.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm.notebook import tqdm\ntrain = np.asarray(train[['signal','open_channels']])\ntest_dict = {}\nfor sig,chan in tqdm(test):\n    temp = []\n    try:\n        temp = train_dict[chan]\n        temp.append(sig)\n    except KeyError:\n        temp.append(sig)\n    finally:\n        train_dict[chan] = temp","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## What data was recorded?  \nThe electrical signal and maximum number of channels open constitute a data instance.\n## How was the data collected? \nOne data instance is recorded every 0.1 milli-second. So, in one second there are 10,000 data instances recorded.  \nThe data was recorded in batches of 50 seconds. Therefore, one batch contains 500,000 rows.  \nTraining data contains 10 batches: 5 million rows  \nTest data contains 4 batches: 2 million rows  "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,5)); res = 1000\nplt.xticks(np.arange(0, 5500000, step=500000))\nplt.plot(range(0,train.shape[0],res),train.signal[0::res])\nfor i in range(11): plt.plot([i*500000,i*500000],[-5,12.5],'r')\nfor j in range(10): plt.text(j*500000+200000,10,str(j+1),size=20)\nplt.xlabel('Row',size=16); plt.ylabel('Signal',size=16); \nplt.title('Training Data Signal - 10 batches',size=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Can I rearrange the batches? Is batch-1 collected prior to batch-2 and so on & so forth? In other words, Does markovian property apply at batch levels(obvious, 50 secs) or at the training data set level?Since, low-probability channels appear only in the first 2 batches, does that mean that for a channel to become a high-probability channel it needs to first become a low-probability one and then transcend to become a high-probability one?"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['prev'] = 0\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['prev'][0+1:500000] = train['signal'][0:500000-1]\ntrain['prev'][500000+1:1000000] = train['signal'][500000:1000000-1]\ntrain['prev'][1000000+1:1500000] = train['signal'][1000000:1500000-1]\ntrain['prev'][1500000+1:2000000] = train['signal'][1500000:2000000-1]\ntrain['prev'][2000000+1:2500000] = train['signal'][2000000:2500000-1]\ntrain['prev'][2500000+1:3000000] = train['signal'][2500000:3000000-1]\ntrain['prev'][3000000+1:3500000] = train['signal'][3000000:3500000-1]\ntrain['prev'][3500000+1:4000000] = train['signal'][3500000:4000000-1]\ntrain['prev'][4000000+1:4500000] = train['signal'][4000000:4500000-1]\ntrain['prev'][4500000+1:5000000] = train['signal'][4500000:5000000-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(train['time'][500000:1000000],train['signal'][500000:1000000])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=train.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#REMOVING OUTLIERS: POINTS OUTSIDE 3SD\ntemp = []\nfor i in range(10): \n    print('Processing Batch-{}'.format(i+1))\n    a = i * 500000\n    b = (i+1) * 500000\n    temp_df = df[a:b]\n    plt.plot(temp_df['open_channels'],temp_df['signal'])\n    plt.show()\n    temp_df = temp_df[np.abs(temp_df.signal-temp_df.signal.mean()) <= (2*temp_df.signal.std())]\n    print('Aftre removing outliers in Batch-{}'.format(i+1))\n    plt.plot(temp_df['open_channels'],temp_df['signal'])\n    plt.show()\n    temp.append(temp_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_df = pd.DataFrame(np.vstack(temp))\nnew_df.columns = ['time','signal','open_channels']\nnew_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,5)); res = 1000\nplt.ylim(bottom=-1);plt.ylim(top=12)\nplt.yticks(np.arange(-1, 12, step=1))\nplt.xticks(np.arange(0, 5500000, step=500000))\nplt.plot(range(0,train.shape[0],res),train.open_channels[0::res])\nfor i in range(11): plt.plot([i*500000,i*500000],[-5,12.5],'r')\nfor j in range(10): plt.text(j*500000+200000,10,str(j+1),size=20)\nplt.xlabel('Row',size=16); \nplt.ylabel('Channels Open',size=16); \nplt.title('Training Data Open Channels - 10 batches',size=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The channels are classified broadly into 2 categories depending on whether they have a low-probability of opening(low conductance) or a high-probability of opening(high conductance):-  \n1.Batch1 and Batch2 represent low-probability channels: binary classification.>>>>Model1  \n2.Other Batches represnet high-probabaility channels: multi-class classification.>>>>Model2"},{"metadata":{},"cell_type":"markdown","source":"## Analysis from above EDA:-  \nFrom the plots above, it looks like they used 5 different synthetic models.   \nOne model produced maximum 1 open channel with low probability (batches 1 and 2).   \nOne model produced maximum 1 open channel with high probability (batches 3 and 7).   \nOne model produced maximum 3 open channels (batches 4 and 8).   \nOne model produced maximum 5 open channels (batches 6 and 9) and  \nOne model produced maximum 10 open channels (batches 5 and 10).   \n\nAccording to the paper [here][1], the data is synthesized. Also \"electrophysiological\" noise and drift were added.  \nDrift is a signal bias causing the signal to no longer be a horizontal line like batches 2, 7, 8, 9, 10.\n\n> Data description and dataset construction. Ion channel dwell-times were\nsimulated using the method of Gillespie 43 from published single channel models.\nChannels are assumed to follow a stochastic Markovian process and transition\nfrom one state to the next simulated by randomly sampling from a lifetime\nprobability distribution calculated for each state. Authentic “electrophysiological”\nnoise was added to these events by passing the signal through a patch-clamp\namplifier and recording it back to file with CED’s Signal software via an Axon\nelectronic “model cell”. In some datasets additional drift was applied to the final\ndata with Matlab. Two different stochastic gating models, (termed M1 and M2)\nwere used to generate semi-synthetic ion channel data. M1 is a low open probability model from ref. 41 (Fig. 3a, b), typically no more than one ion channel opens\nsimultaneously. Model M2 is from refs. 42,44 and has a much higher open probability (Fig. 3c, d), consequently up to five channels opened simultaneously and there are few instances of zero channels open.\n\n\n[1]: https://www.nature.com/articles/s42003-019-0729-3\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(train['open_channels'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Correlation Between Signal and Open Channels\nLet's look closely at random intervals of signal and open channels to observe how they relate. We notice that they are highly correlated and move up and down together. Therefore we can probabily predict open channels from the one feature signal. The only complication is the synthetic drift that was added."},{"metadata":{"trusted":true},"cell_type":"code","source":"for k in range(10):\n    a = int( np.random.uniform(0,train.shape[0]-50000) )\n    b=a+5000; res=10\n    print('#'*25)\n    print('### Random %i to %i'%(a,b))\n    print('#'*25)\n    plt.figure(figsize=(20,5))\n    plt.plot(range(a,b,res),train.signal[a:b][0::res])\n    plt.plot(range(a,b,res),train.open_channels[a:b][0::res])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Test Data\nLet's display the test data signal"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,5))\nplt.xticks(np.arange(0, 2500000, step=100000))\nres = 1000; let = ['1s', '3', '5', '1s','1f','10','5','10','1s','3']\nplt.plot(range(0,test.shape[0],res),test.signal[0::res])\nfor i in range(5): plt.plot([i*500000,i*500000],[-5,12.5],'r')\nfor j in range(21): plt.plot([j*100000,j*100000],[-5,12.5],'y:')\nfor k in range(4): plt.text(k*500000+200000,10,str(k+1),size=20)\nfor k in range(10): plt.text(k*100000+40000,7,let[k],size=16)\nplt.xlabel('Row',size=16); plt.ylabel('Signal',size=16); \nplt.title('Test Data Signal - 4 batches - 10 subsamples',size=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Analysis of EDA above:-  \nFrom this plot we can locate the 5 models in action. "},{"metadata":{},"cell_type":"markdown","source":"## Nizamuddin approach: Make 2 models:-  \n1.for low-probability channels : batch1,batch2 in training data  \n2.for high-probability channels: batch3 to batch10 in training data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train2 = train.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1) low probability model"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = np.asarray(train2[['signal','prev']][0:1000000]).reshape((-1,2))\ny_train = np.asarray(train2.open_channels.values[0:1000000]).reshape((-1,1))\nprint('X_train.shape,y_train.shape:',X_train.shape,y_train.shape)\nplt.hist(X_train)\nplt.hist(y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf1s = tree.DecisionTreeClassifier(max_depth=1,criterion='entropy')\nclf1s = clf1s.fit(X_train,y_train)\nprint('Training model low-probability channel')\npreds = clf1s.predict(X_train)\nprint('f1 validation score =',f1_score(y_train,preds,average='macro'))\ntree_graph = tree.export_graphviz(clf1s, out_file=None, max_depth = 10,\n    impurity = False, feature_names = ['signal','prev'], class_names = ['0', '1'],\n    rounded = True, filled= True )\ngraphviz.Source(tree_graph)  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2)high probability model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n# import pandas\n# import xgboost\n# from sklearn import model_selection\n# from sklearn.metrics import accuracy_score\n# from sklearn.preprocessing import LabelEncoder\n\n# X = np.asarray(train2[['signal','prob']][1000000:]).reshape((-1,2))\n# Y = np.asarray(train2.open_channels.values[1000000:])\n# print('X_train.shape,y_train.shape:',X_train.shape,y_train.shape)\n# # encode string class values as integers\n# label_encoder = LabelEncoder()\n# label_encoder = label_encoder.fit(Y)\n# label_encoded_y = label_encoder.transform(Y)\n# seed = 7\n# test_size = 0.33\n# X_train, X_test, y_train, y_test = model_selection.train_test_split(X, label_encoded_y, test_size=test_size, random_state=seed)\n# # fit model no training data\n# model = xgboost.XGBClassifier(objective='multi:softmax',num_classes=11)\n# model.fit(X_train, y_train)\n# print(model)\n# # make predictions for test data\n# y_pred = model.predict(X_test)\n# predictions = [round(value) for value in y_pred]\n# # evaluate predictions\n# accuracy = accuracy_score(y_test, predictions)\n# print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from numpy import argmax\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nX_train = np.asarray(train2[['signal','prev']][1200000:]).reshape((-1,2))\ny_train = np.asarray(train2.open_channels.values[1200000:])\nprint('X_train.shape,y_train.shape:',X_train.shape,y_train.shape,type(X_train))\n\nX = X_train\ny = y_train\nseed = 1\n#y = LabelEncoder().fit_transform(y)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30,random_state=seed)\nX_train, X_test, y_train, y_test = train_test_split(X_test, y_test, test_size=0.20,random_state=seed)\n# X_train, X_test, y_train, y_test = train_test_split(X_test, y_test, test_size=0.25,random_state=seed)\n\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\nsns.countplot(y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x=4\n# X_train = X_train.reshape((-1,x,2))\n# X_test = X_test.reshape((-1,x,2))\n# y_train = y_train.reshape((-1,x,1))\n# y_test = y_test.reshape((-1,x,1))\n# y_train=np.asarray([*map(np.squeeze,y_train)])\n# y_test=np.asarray([*map(np.squeeze,y_test)])\n# y_train=y_train.reshape(-1,1,4)\n# y_test=y_test.reshape(-1,1,4)\n# print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For seed=1,1 dense layer 0f 10 units 80% accuracy"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nimport tensorflow\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import Conv1D\nfrom tensorflow.keras.layers import MaxPool1D\n# determine the number of input features\nn_timesteps = x\nn_features = 2\nprint('n_timesteps: ',n_timesteps,'n_features:',n_features)\nn_classes = 11   #0,1,2.....10\nmodel = Sequential()\n# model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_timesteps,n_features)))\n# #model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n# model.add(Dropout(0.25))\n# model.add(MaxPool1D(pool_size=2))\n# model.add(Flatten())\nmodel.add(Dense(10, activation='relu',input_shape=(n_features,)))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(n_classes, activation='softmax'))\nmodel.compile(optimizer='adam',\n              loss=\"sparse_categorical_crossentropy\",\n              metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train\nhistory = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n# evaluate the model\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss, acc = model.evaluate(X_test, y_test, verbose=1)\nprint('Test Accuracy: %.4f' % acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ny_pred = model.predict(X_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make a prediction\n\nprint('Predicted: %s (class=%d)' % (yhat, argmax(yhat)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Dense, Embedding, Dropout, LSTM\nfrom keras.models import Sequential\nfrom keras.layers import Bidirectional\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers.convolutional import MaxPooling1D\n\nembed_dim = 100\nlstm_out = 128\nmax_features = 5000\n\nmodel8 = Sequential()\nmodel8.add(Embedding(max_features, embed_dim, input_length = X_train.shape[0]))\nmodel8.add(Dropout(0.2))\nmodel8.add(Conv1D(filters=100, kernel_size=3, padding='same',  activation='relu'))\nmodel8.add(MaxPooling1D(pool_size=2))\nmodel8.add(Bidirectional(LSTM(lstm_out)))\nmodel8.add(Dense(n_classes,activation='softmax'))\nmodel8.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\nprint(model8.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model8.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Make Five Simple Models\nWe will make one model for each different type of signal we observed above."},{"metadata":{"trusted":true},"cell_type":"code","source":"train2 = train.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1 Slow Open Channel--batch1 and batch2"},{"metadata":{"trusted":true},"cell_type":"code","source":"batch = 1; a = 500000*(batch-1); b = 500000*batch\nbatch = 2; c = 500000*(batch-1); d = 500000*batch\nX_train = np.concatenate([train2.signal.values[a:b],train2.signal.values[c:d]]).reshape((-1,1))\ny_train = np.concatenate([train2.open_channels.values[a:b],train2.open_channels.values[c:d]]).reshape((-1,1))\n\nclf1s = tree.DecisionTreeClassifier(max_depth=1)\nclf1s = clf1s.fit(X_train,y_train)\nprint('Training model 1s channel')\npreds = clf1s.predict(X_train)\nprint('has f1 validation score =',f1_score(y_train,preds,average='macro'))\nprint('X_train.shape,y_train.shape: ',X_train.shape,y_train.shape)\ntree_graph = tree.export_graphviz(clf1s, out_file=None, max_depth = 10,\n    impurity = False, feature_names = ['signal'], class_names = ['0', '1'],\n    rounded = True, filled= True )\ngraphviz.Source(tree_graph)  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1 Fast Open Channel"},{"metadata":{"trusted":true},"cell_type":"code","source":"batch = 3; a = 500000*(batch-1); b = 500000*batch\nbatch = 7; c = 500000*(batch-1); d = 500000*batch\nX_train = np.concatenate([train2.signal.values[a:b],train2.signal.values[c:d]]).reshape((-1,1))\ny_train = np.concatenate([train2.open_channels.values[a:b],train2.open_channels.values[c:d]]).reshape((-1,1))\n\nclf1f = tree.DecisionTreeClassifier(max_depth=1)\nclf1f = clf1f.fit(X_train, y_train)\nprint('Training model 1f channel')\npreds = clf1f.predict(X_train)\nprint('has f1 validation score =',f1_score(y_train,preds,average='macro'))\n\ntree_graph = tree.export_graphviz(clf1f, out_file=None, max_depth = 10,\n    impurity = False, feature_names = ['signal'], class_names = ['0', '1'],\n    rounded = True, filled= True )\ngraphviz.Source(tree_graph) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3 Open Channels"},{"metadata":{"trusted":true},"cell_type":"code","source":"batch = 4; a = 500000*(batch-1); b = 500000*batch\nbatch = 8; c = 500000*(batch-1); d = 500000*batch\nX_train = np.concatenate([train2.signal.values[a:b],train2.signal.values[c:d]]).reshape((-1,1))\ny_train = np.concatenate([train2.open_channels.values[a:b],train2.open_channels.values[c:d]]).reshape((-1,1))\n\nclf3 = tree.DecisionTreeClassifier(max_leaf_nodes=4)\nclf3 = clf3.fit(X_train,y_train)\nprint('Training model 3 channel')\npreds = clf3.predict(X_train)\nprint('has f1 validation score =',f1_score(y_train,preds,average='macro'))\n\ntree_graph = tree.export_graphviz(clf3, out_file=None, max_depth = 10,\n    impurity = False, feature_names = ['signal'], class_names = ['0', '1','2','3'],\n    rounded = True, filled= True )\ngraphviz.Source(tree_graph) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5 Open Channels"},{"metadata":{"trusted":true},"cell_type":"code","source":"batch = 6; a = 500000*(batch-1); b = 500000*batch\nbatch = 9; c = 500000*(batch-1); d = 500000*batch\nX_train = np.concatenate([train2.signal.values[a:b],train2.signal.values[c:d]]).reshape((-1,1))\ny_train = np.concatenate([train2.open_channels.values[a:b],train2.open_channels.values[c:d]]).reshape((-1,1))\n\nclf5 = tree.DecisionTreeClassifier(max_leaf_nodes=6)\nclf5 = clf5.fit(X_train, y_train)\nprint('Trained model 5 channel')\npreds = clf5.predict(X_train)\nprint('has f1 validation score =',f1_score(y_train,preds,average='macro'))\n\ntree_graph = tree.export_graphviz(clf5, out_file=None, max_depth = 10,\n    impurity = False, feature_names = ['signal'], class_names = ['0', '1','2','3','4','5'],\n    rounded = True, filled= True )\ngraphviz.Source(tree_graph) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 10 Open Channels"},{"metadata":{"trusted":true},"cell_type":"code","source":"batch = 5; a = 500000*(batch-1); b = 500000*batch\nbatch = 10; c = 500000*(batch-1); d = 500000*batch\nX_train = np.concatenate([train2.signal.values[a:b],train2.signal.values[c:d]]).reshape((-1,1))\ny_train = np.concatenate([train2.open_channels.values[a:b],train2.open_channels.values[c:d]]).reshape((-1,1))\n\nclf10 = tree.DecisionTreeClassifier(max_leaf_nodes=8)\nclf10 = clf10.fit(X_train, y_train)\nprint('Trained model 10 channel')\npreds = clf10.predict(X_train)\nprint('has f1 validation score =',f1_score(y_train,preds,average='macro'))\n\ntree_graph = tree.export_graphviz(clf10, out_file=None, max_depth = 10,\n    impurity = False, feature_names = ['signal'], class_names = [str(x) for x in range(11)],\n    rounded = True, filled= True )\ngraphviz.Source(tree_graph) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Analyze Test Data Drift\nLet's plot the drift in the training and test data"},{"metadata":{},"cell_type":"markdown","source":"## Training Data Drift\nWe observe drift whereever the following plot is not a horizontal line. We see drift in batches 2, 7, 8, 9, 10."},{"metadata":{"trusted":true},"cell_type":"code","source":"# ORIGINAL TRAIN DATA\nplt.figure(figsize=(20,5))\nr = train.signal.rolling(30000).mean()\nplt.plot(train.time.values,r)\nfor i in range(11): plt.plot([i*50,i*50],[-3,8],'r:')\nfor j in range(10): plt.text(j*50+20,6,str(j+1),size=20)\nplt.title('Training Signal Rolling Mean. Has Drift wherever plot is not horizontal line',size=16)\nplt.show()\n\n# TRAIN DATA WITHOUT DRIFT\nplt.figure(figsize=(20,5))\nr = train2.signal.rolling(30000).mean()\nplt.plot(train2.time.values,r)\nfor i in range(11): plt.plot([i*50,i*50],[-3,8],'r:')\nfor j in range(10): plt.text(j*50+20,6,str(j+1),size=20)\nplt.title('Training Signal Rolling Mean without Drift',size=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Test Data Drift\nWe observe drift in test subsamples A, B, E, G, H, I and test batch 3.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,5))\nlet = ['A','B','C','D','E','F','G','H','I','J']\nr = test2.prev.rolling(30000).mean()\nplt.plot(test.time.values,r)\nfor i in range(21): plt.plot([500+i*10,500+i*10],[-3,6],'r:')\nfor i in range(5): plt.plot([500+i*50,500+i*50],[-3,6],'r')\nfor k in range(4): plt.text(525+k*50,5.5,str(k+1),size=20)\nfor k in range(10): plt.text(505+k*10,4,let[k],size=16)\nplt.title('Test Signal Rolling Mean. Has Drift wherever plot is not horizontal line',size=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predict Test\n"},{"metadata":{},"cell_type":"markdown","source":"1s ---> atmost 1 open channel with low prob  \n1f ---> atmost 1 open channel with high prob  \n 3 ---> atmost 3 open channels with high prob  \n 5 ---> atmost 5 open channels with high prob  \n10 ---> atmost 10 open channels with high prob  "},{"metadata":{"trusted":true},"cell_type":"code","source":"test2.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test2=test.copy()\ntest2['prev']=0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test2['prev'][0+1:500000] = test2['signal'][0:500000-1]\ntest2['prev'][500000+1:1000000] = test2['signal'][500000:1000000-1]\ntest2['prev'][1000000+1:1500000] = test2['signal'][1000000:1500000-1]\ntest2['prev'][1500000+1:2000000] = test2['signal'][1500000:2000000-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test2['prob']=0.5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a = 0\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv('../input/liverpool-ion-switching/sample_submission.csv')\n\na = 0 # SUBSAMPLE A, Model 1s\n#sub.iloc[100000*a:100000*(a+1),1] = clf1s.predict(test2[['signal','prev']][100000*a:100000*(a+1)].reshape((-1,1)))\nsub.iloc[100000*a:100000*(a+1),1] =clf1s.predict(np.asarray(test2[['signal','prev']][100000*a:100000*(a+1)]))\n\na = 1 # SUBSAMPLE B, Model 3\ny_pred = model.predict(np.asarray(test2[['signal','prev']][100000*a:100000*(a+1)]))\npredictions = [*map(np.argmax,y_pred)]\nsub.iloc[100000*a:100000*(a+1),1] = np.asarray(predictions)\n\na = 2 # SUBSAMPLE C, Model 5\ny_pred = model.predict(np.asarray(test2[['signal','prev']][100000*a:100000*(a+1)]))\npredictions =[*map(np.argmax,y_pred)]\nsub.iloc[100000*a:100000*(a+1),1] = np.asarray(predictions)\n\na = 3 # SUBSAMPLE D, Model 1s\n#sub.iloc[100000*a:100000*(a+1),1] = clf1s.predict(test2.signal.values[100000*a:100000*(a+1)].reshape((-1,1)))\nsub.iloc[100000*a:100000*(a+1),1] =clf1s.predict(np.asarray(test2[['signal','prev']][100000*a:100000*(a+1)]))\n\na = 4 # SUBSAMPLE E, Model 1f\ny_pred = model.predict(np.asarray(test2[['signal','prev']][100000*a:100000*(a+1)]))\npredictions = [*map(np.argmax,y_pred)]\nsub.iloc[100000*a:100000*(a+1),1] = np.asarray(predictions)\n\na = 5 # SUBSAMPLE F, Model 10\ny_pred = model.predict(np.asarray(test2[['signal','prev']][100000*a:100000*(a+1)]))\npredictions = [*map(np.argmax,y_pred)]\nsub.iloc[100000*a:100000*(a+1),1] = np.asarray(predictions)\n\na = 6 # SUBSAMPLE G, Model 5\ny_pred = model.predict(np.asarray(test2[['signal','prev']][100000*a:100000*(a+1)]))\npredictions = [*map(np.argmax,y_pred)]\nsub.iloc[100000*a:100000*(a+1),1] = np.asarray(predictions)\n\na = 7 # SUBSAMPLE H, Model 10\ny_pred = model.predict(np.asarray(test2[['signal','prev']][100000*a:100000*(a+1)]))\npredictions = [*map(np.argmax,y_pred)]\nsub.iloc[100000*a:100000*(a+1),1] = np.asarray(predictions)\n\na = 8 # SUBSAMPLE I, Model 1s\n#sub.iloc[100000*a:100000*(a+1),1] = clf1s.predict(test2.signal.values[100000*a:100000*(a+1)].reshape((-1,1)))\nsub.iloc[100000*a:100000*(a+1),1] =clf1s.predict(np.asarray(test2[['signal','prev']][100000*a:100000*(a+1)]))\n\na = 9 # SUBSAMPLE J, Model 3\ny_pred = model.predict(np.asarray(test2[['signal','prev']][100000*a:100000*(a+1)]))\npredictions = [*map(np.argmax,y_pred)]\nsub.iloc[100000*a:100000*(a+1),1] = np.asarray(predictions)\n\n # BATCHES 3 AND 4 seem to be generated from Model 1s\n#sub.iloc[1000000:2000000,1] = clf1s.predict(test2.signal.values[1000000:2000000].reshape((-1,1)))\nsub.iloc[1000000:2000000,1] =clf1s.predict(np.asarray(test2[['signal','prev']][1000000:2000000]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Display Test Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,5))\nres = 1000\nplt.plot(range(0,test.shape[0],res),sub.open_channels[0::res])\nfor i in range(5): plt.plot([i*500000,i*500000],[-5,12.5],'r')\nfor i in range(21): plt.plot([i*100000,i*100000],[-5,12.5],'r:')\nfor k in range(4): plt.text(k*500000+250000,10,str(k+1),size=20)\nfor k in range(10): plt.text(k*100000+40000,7.5,let[k],size=16)\nplt.title('Test Data Predictions',size=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv('submission.csv',index=False,float_format='%.4f')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub['open_channels'].hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\n# save model to file\npickle.dump(model, open(\"pima.pickle.dat\", \"wb\"))\n \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# some time later...\n \n# load model from file\nloaded_model = pickle.load(open(\"pima.pickle.dat\", \"rb\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loaded_model.predict(X_test[:10])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}